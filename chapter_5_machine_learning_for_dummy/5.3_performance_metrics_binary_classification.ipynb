{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5.3 Performance metrics for binary classification models\n",
    "After building and training the prediction models for classification, it is important to evaluate them properly to see how well they perform in predicting the class of new unseen data. \n",
    "\n",
    "There are several metrics to evaluate the classification models as this domain has received a lot of attention from researchers and practitioners. Accordingly, there is a wide variety of model performance metrics to choose from.\n",
    "\n",
    "> Note: For an idea of the range of options, have a look at the scikit-learn model evaluation page: https://scikit-learn.org/stable/modules/model_evaluation.html#model-evaluation\n",
    "\n",
    "When selecting a model performance metric to assess the predictive quality of a model, it's important to keep two things in mind: \n",
    "\n",
    "**The suitability of the metric for the tackled problem.**\n",
    "\n",
    "Metrics are typically only defined for a specific class of problems, such as classification or regression. \n",
    "\n",
    "- For a binary classification problem, several metrics identify the correctness of the yes or no question that the model answers. An additional level of detail here is how often the model is correct for each class, the positive and negative classes. We will go into detail on these metrics here. \n",
    "- For a regression problem, however, we use other regression metrics. We aim at measuring how close a prediction is to the target quantity (continuous value). For example, If we are trying to predict the exchange rate of a specific currency, we need to know, for example, to what extent we are getting predicted exchange rates that are close to the actual exchange rates.\n",
    "\n",
    "**The metric ability to address the business problem.**\n",
    "\n",
    "When choosing the right metric to evaluate a specific model for a specific business problem, we can be more objective in our selection when we relate our metric selection to the tackled business problem. This could be done via relating this to benefit or profit achieved when we are able to evaluate the model performance correctly. \n",
    "\n",
    "For example, in the credit card fraud transaction detection problem, we can as the following questions: is there a high cost associated with not correctly identifying transactions that are frauds? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Common metrics used to assess the prediction quality of binary classification models.\n",
    "\n",
    "**Classification accuracy**\n",
    "\n",
    "Accuracy is defined as the proportion of samples that were correctly classified. For example, in our case study: \n",
    "- if our classification model predicts the response variable (Class_Category) for 10 transactions as follows:  `y_pred = [1100101010]`  \n",
    "- the actual values of the response variable for the same 10 transactions are `y_actual = [1110101110]`\n",
    "- We can say that our model is able to predict correctly the response variable of 8 transactions out of 10 transactions meaning that it achieves 8/10=80% accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**True positive rate**\n",
    "\n",
    "In binary classification, there are only two labels to consider: positive and negative. A better way to evaluate model performance than the accuracy of prediction across all samples is to evaluate the accuracy of only samples that have a positive label.\n",
    "\n",
    "The number of positive samples that were correctly predicted to be positive by the model are defined as `True positives (TP)`. \n",
    "\n",
    "Given that `P` is the number of samples in the positive class in the test data, and TP is the number of true positives, the proportion of samples that we successfully predicted as positive is called the `true positive rate (TPR)`. TPR is calculated as follows: $TPR = \\frac{TP}{P}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**False negative rate**\n",
    "\n",
    "The number of positive samples that were incorrectly predicted as negative by the model are defined as `False Negatives (FN)`. \n",
    "\n",
    "The `false negative rate (FNR)` is the proportion of positive test samples that we incorrectly predicted as negative. The `false negative rate (FNR)` is calculated as: $FNR = \\frac{FN}{P}$\n",
    "\n",
    "The sum of the number of true positives and the number of false negatives equals the total number of positive samples; since all the positive samples are either correctly or incorrectly predicted. Mathematically: $P=TP+FN$\n",
    "\n",
    "therefore, using the definitions of TPR and FNR, we have: $TPR+FNR=1$\n",
    "\n",
    "Since the TPR and FNR sum to 1, it's sufficient to just calculate one of them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**True negative and false positive rates**\n",
    "\n",
    "The number of negative samples that were correctly predicted to be negative by the model are defined as `True Negative (TN)`. If `N` is the number of negative samples, the `true negative rate (TNR)` is the proportion of samples that we successfully predicted as negative. \n",
    "\n",
    "$TNR = \\frac{TN}{N}$\n",
    "\n",
    "Similarly, the number of negative samples that were incorrectly predicted to be positive by the model are defined as `False Positive (FP)`.  the `false positive rate (FPR)` is the proportion of samples that we incorrectly predicted as positive. \n",
    "\n",
    "$FPR = \\frac{FP}{N}$\n",
    "\n",
    "The sum of the number of true negatives and the number of false positives equals the total number of negative samples; since all the negative samples are either correctly or incorrectly predicted. Mathematically: $N=TN+FP$\n",
    "\n",
    "therefore, using the definitions of TPR and FNR, we have: $TNR + FPR = 1$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Confusion matrix**\n",
    "\n",
    "`True negatives (TN)`, `False Negatives (FN)`, `False Positives (FP)`, and `True Positives (TP)` can be summarised in a table called a confusion matrix. A confusion matrix for a binary classification problem is a `2 x 2 matrix` where the `true` class is along one axis and the `predicted` class is along the other axis. \n",
    "\n",
    "\n",
    "<style type=\"text/css\">\n",
    "  .tg  {border-collapse:collapse;border-spacing:0;}\n",
    "  .tg td{border-color:black;border-style:solid;border-width:1px;font-family:Arial, sans-serif;font-size:14px;\n",
    "    overflow:hidden;padding:10px 5px;word-break:normal;}\n",
    "  .tg th{border-color:black;border-style:solid;border-width:1px;font-family:Arial, sans-serif;font-size:14px;\n",
    "    font-weight:normal;overflow:hidden;padding:10px 5px;word-break:normal;}\n",
    "  .tg .tg-0pky{border-color:inherit;text-align:left;vertical-align:top}\n",
    "  .tg .tg-0lax{text-align:left;vertical-align:top}\n",
    "</style>\n",
    "<table class=\"tg\">\n",
    "  <thead>\n",
    "    <tr>\n",
    "      <th class=\"tg-0pky\" colspan=\"2\" rowspan=\"2\"></th>\n",
    "      <th class=\"tg-0pky\" colspan=\"2\">Predicted Class</th>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <th class=\"tg-0pky\">N</th>\n",
    "      <th class=\"tg-0lax\">P</th>\n",
    "    </tr>\n",
    "  </thead>\n",
    "  <tbody>\n",
    "    <tr>\n",
    "      <td class=\"tg-0pky\" rowspan=\"2\">True Class</td>\n",
    "      <td class=\"tg-0pky\">N</td>\n",
    "      <td class=\"tg-0pky\">TN</td>\n",
    "      <td class=\"tg-0lax\">FP</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <td class=\"tg-0lax\">P</td>\n",
    "      <td class=\"tg-0lax\">FN</td>\n",
    "      <td class=\"tg-0lax\">TP</td>\n",
    "    </tr>\n",
    "  </tbody>\n",
    "</table>\n",
    "\n",
    "Since we hope to make correct classifications, we hope that the diagonal entries (the entries along a diagonal line from the top left to the bottom right: `TN` and `TP`) of the confusion matrix are relatively large, while the off-diagonal entries (`FP` and `FN`) are relatively small, as these represent incorrect classifications. \n",
    "\n",
    "- The accuracy metric can be calculated from the confusion matrix by adding up the entries on the diagonal, which are predictions that are correct, and dividing by the total number of all predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Precision**\n",
    "\n",
    "Precision quantifies the number of positive class predictions that actually belong to the positive class. Or in other words, the proportion of positive predictions that are correct.\n",
    "\n",
    "It is calculated as follows: \n",
    "\n",
    "$Precision = \\frac{TP}{TP+FP}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Recall**\n",
    "\n",
    "Recall quantifies the number of positive class predictions made out of all positive samples in the dataset.\n",
    "\n",
    "$Recall = \\frac{TP}{TP+FN}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**F1-score**\n",
    "\n",
    "The F1 Score provides a single score that balances both the concerns of precision and recall in one number. The F1 Score is also called the F Score or the F Measure and is calculated as follows:\n",
    "\n",
    "$2 * \\frac{Precision * Recall}{Precision + Recall}$"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
