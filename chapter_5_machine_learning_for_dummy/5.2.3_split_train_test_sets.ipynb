{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.2.3 Credit card data: Splitting the data: training and test sets\n",
    "### Why do we need the concept of train/test split concept?\n",
    "We already introduced a foundational concept in predictive modeling, which is the concept of using a trained model to make predictions on new data that the model had never \"seen\" before. \n",
    "\n",
    "When creating a model for prediction, we need some kind of measure of how well the model can make predictions on data that were not used to fit the model. This is because in fitting a model, the model becomes \"specialised\" at learning the relationship between features and response on the specific set of labeled data that were used for fitting. \n",
    "- While this is nice, in the end, we want to be able to use the model to make accurate predictions on new, unseen data, for which we don't know the true value of the labels.\n",
    "\n",
    "For example, in our case study, once we deliver the trained model to our clients, they will obtain a new dataset of features similar to the one we have now. However, the features' data will be different in the sense it has new values fr V1..V28, time, and amount. Our client will be using the model with the features' data, to predict whether a transaction is a fraud one or not. \n",
    "\n",
    "It is important to evaluate how well we can anticipate the trained model to predict which transactions are frauds. To do that, we can take our current dataset and split it into two sets:\n",
    "\n",
    "- The training set/ training data.  This consists of samples used to train the model.\n",
    "- The test set/test data. This consists of samples that were not used in training the model. The test data is used to evaluate the model on data not seen or used in the trained model. \n",
    "\n",
    "Evaluating the model on a set of test data shall give an idea of how the model will perform when it is actually used by the client for its intended purpose in solving the business problem (e.g. to make predictions on samples that were not included during the model training).\n",
    "\n",
    "### 1. Train/test split in scikit-learn\n",
    "In this section, we illustrate the concept of train/test split using scikit-learn. We shall use the functionality of train_test_split offered by the scikit-learn library to split the data into 70% for training and 30% for testing.\n",
    "\n",
    "- The percentages of 70% for training and 30% for testing are common to make a data split. The idea is we want enough data for training to build trained models that learn and fit from enough data. \n",
    "- You may consider reducing the size of training data if there is enough data for training and you do not want the training process to take so much computational power or time. \n",
    "- In summary, there is no hard rule to specify the percentages of training/testing data split, but the mentioned percentages are common ones. \n",
    "\n",
    "### 2. Defining features and target/response variables\n",
    "Before we perform the data splitting, we define the input features and the target variable as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(892, 31)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv('../datasets/clean_creditcard.csv')\n",
    "print(df.shape)\n",
    "\n",
    "X = df.drop(['Class_Category'], axis=1)\n",
    "y = df[['Class_Category']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first argument to the train_test_split function is the features X, and the second argument is the response variable y. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are four outputs of the function train_test_split: \n",
    "\n",
    "- The features of the samples in the training set X_train\n",
    "- The features of the samples in the test set X_test\n",
    "- The corresponding response variables (y_train, y_test) of those sets of features X_train and X_test, respectively.\n",
    "\n",
    "The train_test_split function is randomly selecting 30% of the row indices from the dataset and subset out these features and responses as test data, leaving the rest for training. \n",
    "\n",
    "In the above code, we've set test_size to 0.3, or 30%. The size of the training data will be automatically set to the remainder, 70%. \n",
    "\n",
    "In making the train/test split, the random_state parameter is set to a specific value, which is a random number seed. \n",
    "\n",
    "- Using this parameter allows a consistent train/test split across runs of the code. Otherwise, the random splitting procedure would select a different 30% of the data for testing each time the code was run.\n",
    "\n",
    "### 4. Examining the shape of data\n",
    "Let's examine the shapes of our training and test data, as in the following code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(624, 30)\n",
      "(268, 30)\n",
      "(624, 1)\n",
      "(268, 1)\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape)\n",
    "print(X_test.shape)\n",
    "print(y_train.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Checking the nature of data (imbalanced/balanced)\n",
    "Now that we have our training and test data, it's good to make sure the nature of the data is the same between these sets. In particular, is the fraction of the positive class similar? You can observe this in the following output:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5032051282051282\n",
      "0.4925373134328358\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "print(np.mean(y_train))\n",
    "print(np.mean(y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The positive class fractions in the training and test data are both about 50%. This is good, as we can say that the training set is representative of the test set. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
