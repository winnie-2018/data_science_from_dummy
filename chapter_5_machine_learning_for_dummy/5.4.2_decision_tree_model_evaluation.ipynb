{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.4.2 Model evaluation of decision tree\n",
    "In this exercise, we shall evaluate the model built and trained earlier via the performance metrics explained previously. Perform the following steps to do that.\n",
    "\n",
    "0. Import libraries needed and run required steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "df = pd.read_csv('../datasets/clean_creditcard.csv')\n",
    "\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "dt_object = DecisionTreeClassifier(max_depth=3)\n",
    "\n",
    "X = df.drop(['Class_Category'], axis=1)\n",
    "y = df[['Class_Category']]\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=0)\n",
    "\n",
    "dt_object.fit(X_train, y_train.values.ravel())\n",
    "y_pred = dt_object.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Calculate the classification accuracy via Python, as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8955223880597015\n"
     ]
    }
   ],
   "source": [
    "is_correct= y_pred==y_test.values.ravel()\n",
    "print(np.mean(is_correct))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Calculate the classification accuracy via Scikit-learn, as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8955223880597015\n",
      "0.8955223880597015\n"
     ]
    }
   ],
   "source": [
    "print(dt_object.score(X_test,y_test))\n",
    "\n",
    "# or by running this code:\n",
    "\n",
    "from sklearn import metrics\n",
    "print(metrics.accuracy_score(y_test,y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Calculate true and false positive and negative rates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "132\n",
      "109\n",
      "0.8257575757575758\n",
      "23\n",
      "0.17424242424242425\n",
      "136\n",
      "131\n",
      "5\n",
      "the true negative rate is 0.9632352941176471 and the false positive rate is 0.03676470588235294\n"
     ]
    }
   ],
   "source": [
    "P = sum(y_test.values.ravel())\n",
    "print(P)\n",
    "\n",
    "TP = sum( (y_test.values.ravel()==1) & (y_pred==1) )\n",
    "print(TP)\n",
    "\n",
    "TPR = TP/P\n",
    "print(TPR)\n",
    "\n",
    "FN = sum( (y_test.values.ravel()==1) & (y_pred==0) )\n",
    "print(FN)\n",
    "\n",
    "FNR = FN/P\n",
    "print(FNR)\n",
    "\n",
    "N= sum(y_test.values.ravel()==0)\n",
    "print(N)\n",
    "\n",
    "TN= sum((y_test.values.ravel()==0) & (y_pred==0))\n",
    "print(TN)\n",
    "\n",
    "FP = sum((y_test.values.ravel()==0) & (y_pred==1))\n",
    "print(FP)\n",
    "\n",
    "TNR = TN/N\n",
    "FPR = FP/N\n",
    "print('the true negative rate is {} and the false positive rate is {}'. format(TNR,FPR))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Calculate the confusion matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix : \n",
      " [[131   5]\n",
      " [ 23 109]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "print(f\"Confusion Matrix : \\n {confusion_matrix(y_test, y_pred)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. Calculate the precision, recall, and F1 score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.956140350877193\n",
      "0.8257575757575758\n",
      "0.8861788617886179\n"
     ]
    }
   ],
   "source": [
    "Precision = TP/ (TP+FP)\n",
    "print(Precision)\n",
    "\n",
    "Recall = TP/ (TP+FN)\n",
    "print(Recall)\n",
    "\n",
    "F1Score = 2*((Precision * Recall)/(Precision+Recall))\n",
    "print(F1Score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6. Generate the classification report:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Report : \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.85      0.96      0.90       136\n",
      "           1       0.96      0.83      0.89       132\n",
      "\n",
      "    accuracy                           0.90       268\n",
      "   macro avg       0.90      0.89      0.89       268\n",
      "weighted avg       0.90      0.90      0.89       268\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "print(f\"Classification Report : \\n {classification_report(y_test, y_pred)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We shall see from the previous results that the decision tree model performs well with the cleaned data as the accuracy, precision, recall, and f1 score for both classes are relatively good. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
